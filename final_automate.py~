import os


########################## first time run  ##########################################

print ("*** FIRST TIME RUN STARTING . . . ")

os.system("hadoop fs -rmr /user/hduser/multi_training.txt")
os.system("hadoop fs -copyFromLocal /home/aditi/SVM/testdata.txt /user/hduser/multi_training.txt")

print ("### TEST DATA COPIED")
#copying cases and data file to hdfs
os.system("hadoop fs -rmr /user/hduser/cases.txt")
os.system("hadoop fs -copyFromLocal /home/aditi/SVM/cases.txt /user/hduser/")
print ("*** CASES.TXT COPIED TO HDFS")

print ("*** removing previous output directories")
os.system("hadoop fs -rmr /user/hduser/multi_output")
os.system("hadoop fs -rmr /user/hduser/multi_acc8")

print ("### TRAINING PART 1 RUNNING . . .")
#os.system("bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -file /home/aditi/SVM/test.py    -mapper /home/aditi/SVM/test.py -file /home/aditi/SVM/test_reducer.py    -reducer /home/aditi/SVM/test_reducer.py -cacheFile 'hdfs://localhost:54310/user/hduser/cases.txt#cases' -input /user/hduser/testdata_svm.txt -output /user/hduser/multi_output")
os.system("bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -file /home/aditi/SVM/test.py    -mapper /home/aditi/SVM/test.py -file /home/aditi/SVM/test_reducer.py    -reducer /home/aditi/SVM/test_reducer.py -cacheFile 'hdfs://localhost:54310/user/hduser/cases.txt#cases' -input /user/hduser/multi_training.txt -output /user/hduser/multi_output")
#output will be /user/hduser/multi_output/part-00000
#os.system("hadoop fs -ls /user/hduser")
print ("*** TRAINING PART 2 RUNNING . . .")
#os.system("bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -file /home/aditi/SVM/svm_test.py    -mapper /home/aditi/SVM/svm_test.py -file /home/aditi/SVM/svm_test_reducer.py    -reducer /home/aditi/SVM/svm_test_reducer.py -cacheFile 'hdfs://localhost:54310/user/hduser/multi_output/part-00000#SVMvalues' -input /user/hduser/testdata_svm.txt -output /user/hduser/multi_acc3")
os.system("bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -file /home/aditi/SVM/svm_test.py    -mapper /home/aditi/SVM/svm_test.py -file /home/aditi/SVM/svm_test_reducer.py    -reducer /home/aditi/SVM/svm_test_reducer.py -cacheFile 'hdfs://localhost:54310/user/hduser/multi_output/part-00000#SVMvalues' -cacheFile 'hdfs://localhost:54310/user/hduser/cases.txt#cases' -input /user/hduser/multi_training.txt -output /user/hduser/multi_acc8")
#output in /user/hduser/multi_acc3/part-00000


os.system("rm -rf part-00000")
os.system("bin/hadoop fs -copyToLocal /user/hduser/multi_acc8/part-00000 /usr/local/hadoop/")



f = open("part-00000", "r")
words = f.read().split()
posclass = words[0].split(",")
#print posclass
#print len(posclass)
negclass = words[1].split(",")
#print negclass
#print len(negclass)

#write to two files ... 1.) Final output with index number  2.) live_pos_neg_file  with index number

#index = 
index = 1  #change this

print ("### First Enqueue operation to pos neg file queue ")

#writing to posneg queue file ----------------------------------
f1 = open("posneg_queue","a")
f1.write(str(index)+","+"p")
for x in posclass:
	f1.write(","+x)
f1.write(",\n")


f1.write(str(index)+","+"n")
for x in negclass:
	f1.write(","+x)
f1.write(",\n")


#os.system("cat posneg_queue")

print ("Appending Content to Final_result")

#final result --------------------------------------------------- appending
f2 = open("final_result","a")
f2.write(str(index)+","+"p")
for x in posclass:
	f2.write(","+x)
f2.write("\t")


f2.write(str(index)+","+"n")
for x in negclass:
	f2.write(","+x)
f2.write("\t")

f2.write(words[2]+"\t"+words[3]+"\t"+words[4]+"\n")

f.close()
f1.close()
f2.close()
#---------------------------------------

print ("******************** FIRST RUN OVER *********************************")

############################# first time run end ####################################



print ("@@@@@@@@@@@@@@@ WHILE RUN STARTS @@@@@@@@@@@@@@@@@@@@@@@")

i=0
while (1):
	i = i+1
	print ("----------------- Loop Run No ------"+ str(i) +"-----------")
	#dequeue business --------------------------------

	print ("************** Deque Operation performing . . . *******************")
	with open('posneg_queue', 'r') as fin:
	    data = fin.read().splitlines(True)
	if(not data):
		print ("!!!!!!!!!!! Nothing to Dequeue , Voila Process Done ! !!!!!!!!!!!!")
		break
	

	with open('posneg_queue', 'w') as fout:
	    if(len(data)==1 ):
	    	fout.truncate()
	    else:
	    	fout.writelines(data[1:])

	#print data[0]
	#print len(data[0])
	tocases = data[0].split(",")
	tocases1 = tocases[2:-1]
	#print tocases1
	#print len(tocases1)
	argument = ""
	for k in tocases1:
		argument += " "+ k

	#-------------------------------------
	print ("$$$$$$$$$$$$$ Calculating Node Number $$$$$$$$$$$$$$$")
	#set index
	if (tocases[1]=='p'):
		index = int(tocases[0])*2
	if (tocases[1]=='n'):
		index = (int(tocases[0])*2)+1

	print ("______________ Running Cases .py _________________________")
	
	os.system ("python /home/aditi/SVM/cases.py "+ str(len(tocases1)) + argument )
	#print ("python /home/aditi/SVM/cases.py "+ str(len(tocases1)) + argument )
	#copying cases and data file to hdfs
	os.system("hadoop fs -rmr /user/hduser/cases.txt")
	os.system("hadoop fs -copyFromLocal /home/aditi/SVM/cases.txt /user/hduser/")
	print ("*** CASES.TXT COPIED TO HDFS")
	#os.system("hadoop fs -rmr /user/hduser/multi_output/part-00000")
	#os.system("hadoop fs -rmr /user/hduser/multi_acc3/part-00000")
	print ("*** removing previous output directories")
	os.system("hadoop fs -rmr /user/hduser/multi_output")
	os.system("hadoop fs -rmr /user/hduser/multi_acc8")



	print ("### TRAINING PART 1 RUNNING . . .")

	#os.system("bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -file /home/aditi/SVM/test.py    -mapper /home/aditi/SVM/test.py -file /home/aditi/SVM/test_reducer.py    -reducer /home/aditi/SVM/test_reducer.py -cacheFile 'hdfs://localhost:54310/user/hduser/cases.txt#cases' -input /user/hduser/testdata_svm.txt -output /user/hduser/multi_output")
	os.system("bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -file /home/aditi/SVM/test.py    -mapper /home/aditi/SVM/test.py -file /home/aditi/SVM/test_reducer.py    -reducer /home/aditi/SVM/test_reducer.py -cacheFile 'hdfs://localhost:54310/user/hduser/cases.txt#cases' -input /user/hduser/multi_training.txt -output /user/hduser/multi_output")
	#output will be /user/hduser/multi_output/part-00000
	#os.system("hadoop fs -ls /user/hduser")

	print ("### TRAINING PART 2 RUNNING . . .")

	#os.system("bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -file /home/aditi/SVM/svm_test.py    -mapper /home/aditi/SVM/svm_test.py -file /home/aditi/SVM/svm_test_reducer.py    -reducer /home/aditi/SVM/svm_test_reducer.py -cacheFile 'hdfs://localhost:54310/user/hduser/multi_output/part-00000#SVMvalues' -input /user/hduser/testdata_svm.txt -output /user/hduser/multi_acc3")
	os.system("bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -file /home/aditi/SVM/svm_test.py    -mapper /home/aditi/SVM/svm_test.py -file /home/aditi/SVM/svm_test_reducer.py    -reducer /home/aditi/SVM/svm_test_reducer.py -cacheFile 'hdfs://localhost:54310/user/hduser/multi_output/part-00000#SVMvalues' -cacheFile 'hdfs://localhost:54310/user/hduser/cases.txt#cases' -input /user/hduser/multi_training.txt -output /user/hduser/multi_acc8")
	#output in /user/hduser/multi_acc3/part-00000

	os.system("rm -rf part-00000")
	os.system("bin/hadoop fs -copyToLocal /user/hduser/multi_acc8/part-00000 /usr/local/hadoop")

	f = open("part-00000", "r")
	words = f.read().split()
	posclass = words[0].split(",")
	#print posclass
	#print len(posclass)
	negclass = words[1].split(",")
	#print negclass
	#print len(negclass)

	#write to two files ... 1.) Final output with index number  2.) live_pos_neg_file  with index number

	#index = 
	#index = 1  #change this

	#writing to posneg queue file ----------------------------------
	if (len(posclass) > 1):
		print ("### Pos Class Enqueue operation")
		f1 = open("posneg_queue","a")
		f1.write(str(index)+","+"p")
		for x in posclass:
			f1.write(","+x)
		f1.write(",\n")

	if (len(negclass) > 1):
		print ("### Neg Class Enqueue operation")
		f1.write(str(index)+","+"n")
		for x in negclass:
			f1.write(","+x)
		f1.write(",\n")


	#os.system("cat posneg_queue")


	print ("^^^^^^^^^^^^ Appending Result to final_result ^^^^^^^^^^^^^^^^^^^^^^")
	#final result --------------------------------------------------- appending
	f2 = open("final_result","a")
	f2.write(str(index)+","+"p")
	for x in posclass:
		f2.write(","+x)
	f2.write("\t")


	f2.write(str(index)+","+"n")
	for x in negclass:
		f2.write(","+x)
	f2.write("\t")

	f2.write(words[2]+"\t"+words[3]+"\t"+words[4]+"\n")

	f.close()
	f1.close()
	f2.close()
	print (" ---------------------- While Lopp No -----------"+ str(i) +"------------- ENDS")
